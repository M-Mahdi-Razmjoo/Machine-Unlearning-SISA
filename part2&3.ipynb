{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=\"center\">Introduction to Machine Learning - Course Code: 25737</h1>\n<h4 align=\"center\">Instructor: Dr. Amiri</h4>\n<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n<h4 align=\"center\">Project Phase 1</h4>\n<h4 align=\"center\">\nParsa hatami -- Mohammad Mahdi Razmjoo\n</h4>\n<h4 align=\"center\">400100962 -- 400101272</h4>\n","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"center\">Simulation Question 4</h4>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split\n\n# Define the CIFAR10Classifier model\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(32 * 14 * 14, 64)  # Adjusted input size to match the output of conv layers\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# Load CIFAR-10 Data\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n# 80% train and 20% validation\ntrain_size = int(0.8 * len(train_data))\nval_size = len(train_data) - train_size\ntrain_data, val_data = random_split(train_data, [train_size, val_size])\n\ntrain_loader = DataLoader(train_data, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\ndef train_model(model, train_loader, val_loader, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        print(f\"Validation Accuracy: {100 * correct / total}%\")\n\n    return model\n\nbaseline_model = CIFAR10Classifier()\nbaseline_model = train_model(baseline_model, train_loader, val_loader, epochs=10)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-03T03:18:43.599723Z","iopub.execute_input":"2024-07-03T03:18:43.600068Z","iopub.status.idle":"2024-07-03T03:21:14.894786Z","shell.execute_reply.started":"2024-07-03T03:18:43.600038Z","shell.execute_reply":"2024-07-03T03:21:14.893799Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170498071/170498071 [00:15<00:00, 11196080.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1345: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n  warnings.warn(warn_msg)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 1.7366610702514649\nValidation Accuracy: 50.09%\nEpoch 2, Loss: 1.4885362251281737\nValidation Accuracy: 55.15%\nEpoch 3, Loss: 1.3926309808731079\nValidation Accuracy: 58.99%\nEpoch 4, Loss: 1.3188941102981568\nValidation Accuracy: 60.82%\nEpoch 5, Loss: 1.2693265802383422\nValidation Accuracy: 61.73%\nEpoch 6, Loss: 1.2287073127746582\nValidation Accuracy: 63.11%\nEpoch 7, Loss: 1.1855706621170043\nValidation Accuracy: 63.46%\nEpoch 8, Loss: 1.1630538983345031\nValidation Accuracy: 63.65%\nEpoch 9, Loss: 1.1259229259490966\nValidation Accuracy: 64.05%\nEpoch 10, Loss: 1.1030537225723267\nValidation Accuracy: 64.92%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h4 align=\"center\">Simulation Question 5</h4>\n","metadata":{}},{"cell_type":"code","source":"def train_model_with_simple_noise(model, train_loader, val_loader, epochs=10, lr=0.001, noise_factor=0.1):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Adding noise to the gradients\n            for param in model.parameters():\n                if param.grad is not None:\n                    noise = torch.normal(mean=0, std=noise_factor, size=param.grad.shape).to(device)\n                    param.grad += noise\n            \n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n        \n        # Validation\n        model.eval()\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, predicted = torch.max(outputs.data, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        print(f\"Validation Accuracy: {100 * correct / total}%\")\n\n    return model\n\nmodified_model = CIFAR10Classifier()\nmodified_model = train_model_with_simple_noise(modified_model, train_loader, val_loader, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-07-03T03:22:03.655478Z","iopub.execute_input":"2024-07-03T03:22:03.656212Z","iopub.status.idle":"2024-07-03T03:24:37.667499Z","shell.execute_reply.started":"2024-07-03T03:22:03.656181Z","shell.execute_reply":"2024-07-03T03:24:37.666601Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Epoch 1, Loss: 2.225510207557678\nValidation Accuracy: 27.19%\nEpoch 2, Loss: 2.0395797691345217\nValidation Accuracy: 32.53%\nEpoch 3, Loss: 1.9526188655853272\nValidation Accuracy: 35.59%\nEpoch 4, Loss: 1.9016451257705689\nValidation Accuracy: 37.37%\nEpoch 5, Loss: 1.8505986602783202\nValidation Accuracy: 39.63%\nEpoch 6, Loss: 1.8042387203216552\nValidation Accuracy: 41.35%\nEpoch 7, Loss: 1.7757052408218383\nValidation Accuracy: 42.28%\nEpoch 8, Loss: 1.7494818239212035\nValidation Accuracy: 42.94%\nEpoch 9, Loss: 1.724596997833252\nValidation Accuracy: 43.98%\nEpoch 10, Loss: 1.7055449342727662\nValidation Accuracy: 44.41%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h4 align=\"center\">Simulation Question 6</h4>","metadata":{}},{"cell_type":"markdown","source":"for writing this part's code, first i used opacus library and using privateEngine function but it didn't had good results. also the values for the model parameters has many problems (unvalid input errors). then i used noised on gradients which has the reasonable output. after it in the next part i used shadow models for implementations.","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split\n\n# Define the CIFAR10Classifier model\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(32 * 6 * 6, 64)  # Adjusted input size to match the output of conv layers\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# Function to determine the output size of the convolutional layers\ndef get_conv_output_size():\n    model = CIFAR10Classifier()\n    model.eval()\n    with torch.no_grad():\n        dummy_input = torch.randn(1, 3, 32, 32)\n        output = model.conv1(dummy_input)\n        output = F.relu(output)\n        output = model.conv2(output)\n        output = F.relu(output)\n        output = F.max_pool2d(output, 2)\n        output = model.dropout1(output)\n        return output.view(output.size(0), -1).size(1)\n\nconv_output_size = get_conv_output_size()\nprint(\"Convolutional output size:\", conv_output_size)\n\ndef update_fc1_input_size(model, new_input_size):\n    model.fc1 = nn.Linear(new_input_size, 64)\n    return model\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_size = int(0.8 * len(train_data))\nunseen_size = len(train_data) - train_size\nseen_data, unseen_data = random_split(train_data, [train_size, unseen_size])\n\ntrain_loader = DataLoader(seen_data, batch_size=64, shuffle=True)\nunseen_loader = DataLoader(unseen_data, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\ndef train_baseline_model(model, train_loader, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\ndef train_privacy_model(model, train_loader, epochs=10, lr=0.001, noise_multiplier=1.1, max_grad_norm=1.0):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Clip gradients\n            total_norm = torch.norm(torch.stack([torch.norm(p.grad) for p in model.parameters() if p.grad is not None]), 2.0)\n            clip_coef = max_grad_norm / (total_norm + 1e-6)\n            if clip_coef < 1:\n                for p in model.parameters():\n                    if p.grad is not None:\n                        p.grad.data.mul_(clip_coef)\n            \n            # Add noise to gradients\n            for p in model.parameters():\n                if p.grad is not None:\n                    noise = torch.normal(0, noise_multiplier * max_grad_norm, p.grad.shape).to(device)\n                    p.grad.add_(noise)\n            \n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\nbaseline_model = CIFAR10Classifier()\nbaseline_model = update_fc1_input_size(baseline_model, conv_output_size)\n\nbaseline_model = train_baseline_model(baseline_model, train_loader, epochs=10)\n\nprivacy_model = CIFAR10Classifier()\nprivacy_model = update_fc1_input_size(privacy_model, conv_output_size)\n\nprivacy_model = train_privacy_model(privacy_model, train_loader, epochs=10)\n\ndef generate_attack_data(model, data_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    attack_data = []\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            attack_data.append((outputs.cpu().numpy(), labels.cpu().numpy()))\n    return attack_data\n\nseen_attack_data_baseline = generate_attack_data(baseline_model, train_loader)\nunseen_attack_data_baseline = generate_attack_data(baseline_model, unseen_loader)\n\nseen_attack_data_privacy = generate_attack_data(privacy_model, train_loader)\nunseen_attack_data_privacy = generate_attack_data(privacy_model, unseen_loader)\n\nclass AttackerModel(nn.Module):\n    def __init__(self, input_dim):\n        super(AttackerModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef train_attacker_model(attack_data, input_dim, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AttackerModel(input_dim).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for (inputs, labels) in attack_data:\n            inputs, labels = torch.tensor(inputs, dtype=torch.float32).to(device), torch.tensor(labels, dtype=torch.long).to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(attack_data)}\")\n\n    return model\n\ninput_dim = seen_attack_data_baseline[0][0].shape[1]\n\ndef prepare_attack_data(seen_data, unseen_data):\n    x = []\n    y = []\n    for data in seen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(1)  # Seen data is labeled as 1\n    for data in unseen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(0)  # Unseen data is labeled as 0\n    return list(zip(x, y))\n\nattack_data_baseline = prepare_attack_data(seen_attack_data_baseline, unseen_attack_data_baseline)\nattack_data_privacy = prepare_attack_data(seen_attack_data_privacy, unseen_attack_data_privacy)\n\ndef create_attack_loader(attack_data):\n    inputs, labels = zip(*attack_data)\n    inputs = torch.tensor(inputs, dtype=torch.float32)\n    labels = torch.tensor(labels, dtype=torch.long)\n    dataset = torch.utils.data.TensorDataset(inputs, labels)\n    return DataLoader(dataset, batch_size=64, shuffle=True)\n\nattack_loader_baseline = create_attack_loader(attack_data_baseline)\nattack_loader_privacy = create_attack_loader(attack_data_privacy)\n\nattacker_model_baseline = train_attacker_model(attack_loader_baseline, input_dim, epochs=10)\nattacker_model_privacy = train_attacker_model(attack_loader_privacy, input_dim, epochs=10)\n\ndef evaluate_attacker_model(model, attack_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\nmia_accuracy_baseline = evaluate_attacker_model(attacker_model_baseline, attack_loader_baseline)\nmia_accuracy_privacy = evaluate_attacker_model(attacker_model_privacy, attack_loader_privacy)\n\nprint(f\"MIA Accuracy for Baseline Model: {mia_accuracy_baseline}%\")\nprint(f\"MIA Accuracy for Privacy-Enhanced Model: {mia_accuracy_privacy}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:00:49.079857Z","iopub.execute_input":"2024-07-02T22:00:49.080400Z","iopub.status.idle":"2024-07-02T22:05:37.548992Z","shell.execute_reply.started":"2024-07-02T22:00:49.080366Z","shell.execute_reply":"2024-07-02T22:05:37.548070Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Convolutional output size: 6272\nFiles already downloaded and verified\nFiles already downloaded and verified\nEpoch 1, Loss: 1.7678411323547363\nEpoch 2, Loss: 1.5409598583221435\nEpoch 3, Loss: 1.4331507625579833\nEpoch 4, Loss: 1.3591145590782165\nEpoch 5, Loss: 1.3061723586082459\nEpoch 6, Loss: 1.2551678314208985\nEpoch 7, Loss: 1.2168778618812561\nEpoch 8, Loss: 1.1781313047409057\nEpoch 9, Loss: 1.1537385991096496\nEpoch 10, Loss: 1.1280500485420226\nEpoch 1, Loss: 2.3000124996185303\nEpoch 2, Loss: 2.2975273902893067\nEpoch 3, Loss: 2.2972838787078858\nEpoch 4, Loss: 2.3007118530273436\nEpoch 5, Loss: 2.295948947143555\nEpoch 6, Loss: 2.29217572517395\nEpoch 7, Loss: 2.296297233200073\nEpoch 8, Loss: 2.283183654022217\nEpoch 9, Loss: 2.27386955909729\nEpoch 10, Loss: 2.27422406539917\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/898899130.py:196: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  inputs, labels = torch.tensor(inputs, dtype=torch.float32).to(device), torch.tensor(labels, dtype=torch.long).to(device)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 0.5097536654271129\nEpoch 2, Loss: 0.5025043099585091\nEpoch 3, Loss: 0.5016026476307598\nEpoch 4, Loss: 0.501030719188778\nEpoch 5, Loss: 0.5007108262051707\nEpoch 6, Loss: 0.5005047058739016\nEpoch 7, Loss: 0.5002919970189824\nEpoch 8, Loss: 0.500293756423094\nEpoch 9, Loss: 0.5001643371703984\nEpoch 10, Loss: 0.49981699281793723\nEpoch 1, Loss: 0.5047402826264081\nEpoch 2, Loss: 0.5019735755289302\nEpoch 3, Loss: 0.5011106045426005\nEpoch 4, Loss: 0.5009644730850253\nEpoch 5, Loss: 0.5007226658827814\nEpoch 6, Loss: 0.500706170221119\nEpoch 7, Loss: 0.5004310321320048\nEpoch 8, Loss: 0.5001962461587414\nEpoch 9, Loss: 0.4997200621935108\nEpoch 10, Loss: 0.4998288520080659\nMIA Accuracy for Baseline Model: 80.0%\nMIA Accuracy for Privacy-Enhanced Model: 80.004%\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader, random_split, TensorDataset\n\n# Define the CIFAR10Classifier model\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(32 * 6 * 6, 64)  # Adjusted input size to match the output of conv layers\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\ndef get_conv_output_size():\n    model = CIFAR10Classifier()\n    model.eval()\n    with torch.no_grad():\n        dummy_input = torch.randn(1, 3, 32, 32)\n        output = model.conv1(dummy_input)\n        output = F.relu(output)\n        output = model.conv2(output)\n        output = F.relu(output)\n        output = F.max_pool2d(output, 2)\n        output = model.dropout1(output)\n        return output.view(output.size(0), -1).size(1)\n\nconv_output_size = get_conv_output_size()\nprint(\"Convolutional output size:\", conv_output_size)\n\ndef update_fc1_input_size(model, new_input_size):\n    model.fc1 = nn.Linear(new_input_size, 64)\n    return model\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_size = int(0.8 * len(train_data))\nunseen_size = len(train_data) - train_size\nseen_data, unseen_data = random_split(train_data, [train_size, unseen_size])\n\ntrain_loader = DataLoader(seen_data, batch_size=64, shuffle=True)\nunseen_loader = DataLoader(unseen_data, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\ndef train_baseline_model(model, train_loader, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\ndef train_privacy_model(model, train_loader, epochs=10, lr=0.001, noise_multiplier=1.1, max_grad_norm=1.0):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Clip gradients\n            total_norm = torch.norm(torch.stack([torch.norm(p.grad) for p in model.parameters() if p.grad is not None]), 2.0)\n            clip_coef = max_grad_norm / (total_norm + 1e-6)\n            if clip_coef < 1:\n                for p in model.parameters():\n                    if p.grad is not None:\n                        p.grad.data.mul_(clip_coef)\n            \n            # Add noise to gradients\n            for p in model.parameters():\n                if p.grad is not None:\n                    noise = torch.normal(0, noise_multiplier * max_grad_norm, p.grad.shape).to(device)\n                    p.grad.add_(noise)\n            \n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\nbaseline_model = CIFAR10Classifier()\nbaseline_model = update_fc1_input_size(baseline_model, conv_output_size)\n\nbaseline_model = train_baseline_model(baseline_model, train_loader, epochs=10)\n\nprivacy_model = CIFAR10Classifier()\nprivacy_model = update_fc1_input_size(privacy_model, conv_output_size)\n\nprivacy_model = train_privacy_model(privacy_model, train_loader, epochs=10)\n\ndef generate_attack_data(model, data_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    attack_data = []\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            attack_data.append((outputs.cpu().numpy(), labels.cpu().numpy()))\n    return attack_data\n\ndef train_multiple_shadow_models(num_shadow_models, train_loader, val_loader, epochs=10):\n    shadow_models = []\n    for _ in range(num_shadow_models):\n        model = CIFAR10Classifier()\n        model = update_fc1_input_size(model, conv_output_size)\n        model = train_baseline_model(model, train_loader, epochs=epochs)\n        shadow_models.append(model)\n    return shadow_models\n\ndef generate_shadow_attack_data(shadow_models, data_loader):\n    attack_data = []\n    for model in shadow_models:\n        attack_data.extend(generate_attack_data(model, data_loader))\n    return attack_data\n\nnum_shadow_models = 2  \nshadow_models_baseline = train_multiple_shadow_models(num_shadow_models, train_loader, unseen_loader)\nshadow_models_privacy = train_multiple_shadow_models(num_shadow_models, train_loader, unseen_loader)\n\nshadow_attack_data_baseline_seen = generate_shadow_attack_data(shadow_models_baseline, train_loader)\nshadow_attack_data_baseline_unseen = generate_shadow_attack_data(shadow_models_baseline, unseen_loader)\nshadow_attack_data_privacy_seen = generate_shadow_attack_data(shadow_models_privacy, train_loader)\nshadow_attack_data_privacy_unseen = generate_shadow_attack_data(shadow_models_privacy, unseen_loader)\n\ndef prepare_attack_data(seen_data, unseen_data):\n    x = []\n    y = []\n    for data in seen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(1) \n    for data in unseen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(0) \n    return list(zip(x, y))\n\nattack_data_baseline = prepare_attack_data(shadow_attack_data_baseline_seen, shadow_attack_data_baseline_unseen)\nattack_data_privacy = prepare_attack_data(shadow_attack_data_privacy_seen, shadow_attack_data_privacy_unseen)\n\ndef create_attack_loader(attack_data):\n    inputs, labels = zip(*attack_data)\n    inputs = torch.tensor(inputs, dtype=torch.float32)\n    labels = torch.tensor(labels, dtype=torch.long)\n    dataset = torch.utils.data.TensorDataset(inputs, labels)\n    return DataLoader(dataset, batch_size=64, shuffle=True)\n\nattack_loader_baseline = create_attack_loader(attack_data_baseline)\nattack_loader_privacy = create_attack_loader(attack_data_privacy)\n\nclass AttackerModel(nn.Module):\n    def __init__(self, input_dim):\n        super(AttackerModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef train_attacker_model(attack_loader, input_dim, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AttackerModel(input_dim).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(attack_loader)}\")\n\n    return model\n\ninput_dim = attack_loader_baseline.dataset.tensors[0].shape[1]\n\nattacker_model_baseline = train_attacker_model(attack_loader_baseline, input_dim, epochs=10)\nattacker_model_privacy = train_attacker_model(attack_loader_privacy, input_dim, epochs=10)\n\ndef evaluate_attacker_model(model, attack_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\nmia_accuracy_baseline = evaluate_attacker_model(attacker_model_baseline, attack_loader_baseline)\nmia_accuracy_privacy = evaluate_attacker_model(attacker_model_privacy, attack_loader_privacy)\n\nprint(f\"MIA Accuracy for Baseline Model: {mia_accuracy_baseline}%\")\nprint(f\"MIA Accuracy for Privacy-Enhanced Model: {mia_accuracy_privacy}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T04:20:44.708184Z","iopub.execute_input":"2024-07-03T04:20:44.708558Z","iopub.status.idle":"2024-07-03T04:33:42.650378Z","shell.execute_reply.started":"2024-07-03T04:20:44.708528Z","shell.execute_reply":"2024-07-03T04:33:42.649166Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Convolutional output size: 6272\nFiles already downloaded and verified\nFiles already downloaded and verified\nEpoch 1, Loss: 1.7850487289428711\nEpoch 2, Loss: 1.5614835346221925\nEpoch 3, Loss: 1.4628979633331298\nEpoch 4, Loss: 1.3873797872543334\nEpoch 5, Loss: 1.3319367524147034\nEpoch 6, Loss: 1.2843160557746887\nEpoch 7, Loss: 1.252872998905182\nEpoch 8, Loss: 1.2203081357955932\nEpoch 9, Loss: 1.196781807231903\nEpoch 10, Loss: 1.1672297500610351\nEpoch 1, Loss: 2.3024287799835204\nEpoch 2, Loss: 2.2987195854187013\nEpoch 3, Loss: 2.2849382568359373\nEpoch 4, Loss: 2.27986473197937\nEpoch 5, Loss: 2.272332763671875\nEpoch 6, Loss: 2.268441830444336\nEpoch 7, Loss: 2.256930570602417\nEpoch 8, Loss: 2.243988162994385\nEpoch 9, Loss: 2.2472985191345214\nEpoch 10, Loss: 2.2477793170928955\nEpoch 1, Loss: 1.7577148406982421\nEpoch 2, Loss: 1.4715978048324585\nEpoch 3, Loss: 1.355044997406006\nEpoch 4, Loss: 1.2875937500953674\nEpoch 5, Loss: 1.2321864344596862\nEpoch 6, Loss: 1.1887373788833617\nEpoch 7, Loss: 1.159536738395691\nEpoch 8, Loss: 1.1354408043861388\nEpoch 9, Loss: 1.1088163413047791\nEpoch 10, Loss: 1.0882000378608703\nEpoch 1, Loss: 1.7806984703063964\nEpoch 2, Loss: 1.5438874910354614\nEpoch 3, Loss: 1.442892294883728\nEpoch 4, Loss: 1.3709914758682251\nEpoch 5, Loss: 1.32576705493927\nEpoch 6, Loss: 1.2829071039199829\nEpoch 7, Loss: 1.2403021961212157\nEpoch 8, Loss: 1.2057042877197266\nEpoch 9, Loss: 1.1810455589294433\nEpoch 10, Loss: 1.1520477599143981\nEpoch 1, Loss: 1.7780457843780517\nEpoch 2, Loss: 1.5234891576766967\nEpoch 3, Loss: 1.4138578531265258\nEpoch 4, Loss: 1.3581723704338073\nEpoch 5, Loss: 1.3055741223335267\nEpoch 6, Loss: 1.2712331263542176\nEpoch 7, Loss: 1.236038014602661\nEpoch 8, Loss: 1.2048299313545228\nEpoch 9, Loss: 1.1826738386154174\nEpoch 10, Loss: 1.1501595618247986\nEpoch 1, Loss: 1.7771614545822143\nEpoch 2, Loss: 1.5314478673934937\nEpoch 3, Loss: 1.4369982187271118\nEpoch 4, Loss: 1.355082423400879\nEpoch 5, Loss: 1.3039521889686585\nEpoch 6, Loss: 1.2553895820617675\nEpoch 7, Loss: 1.229878234386444\nEpoch 8, Loss: 1.1926124895095824\nEpoch 9, Loss: 1.1687747510910034\nEpoch 10, Loss: 1.1493001761436463\nEpoch 1, Loss: 0.5064442389986108\nEpoch 2, Loss: 0.5016496067465076\nEpoch 3, Loss: 0.5011215560228796\nEpoch 4, Loss: 0.5005598048216551\nEpoch 5, Loss: 0.5005642354907855\nEpoch 6, Loss: 0.5003277065657837\nEpoch 7, Loss: 0.5001393526277707\nEpoch 8, Loss: 0.5000936480683542\nEpoch 9, Loss: 0.4998523176707904\nEpoch 10, Loss: 0.49967498409008265\nEpoch 1, Loss: 0.5056793954383084\nEpoch 2, Loss: 0.5018089615947836\nEpoch 3, Loss: 0.5010541431124365\nEpoch 4, Loss: 0.5007752047009142\nEpoch 5, Loss: 0.5005693424419189\nEpoch 6, Loss: 0.5004475615151174\nEpoch 7, Loss: 0.500311551495233\nEpoch 8, Loss: 0.5003550551826955\nEpoch 9, Loss: 0.5002960220255763\nEpoch 10, Loss: 0.5001488402342842\nMIA Accuracy for Baseline Model: 80.0%\nMIA Accuracy for Privacy-Enhanced Model: 74.6%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<h4 align=\"center\">Simulation Question 7</h4>\n","metadata":{}},{"cell_type":"markdown","source":"without editing the given model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset, random_split\nfrom torchvision import datasets, transforms\n\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(1152, 64)  # Update this after calculating the correct size\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\ndef get_conv_output_size(model):\n    with torch.no_grad():\n        dummy_input = torch.randn(1, 3, 32, 32)\n        output = model.conv1(dummy_input)\n        output = F.relu(output)\n        output = model.conv2(output)\n        output = F.relu(output)\n        output = F.max_pool2d(output, 2)\n        output = model.dropout1(output)\n        return output.view(output.size(0), -1).size(1)\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_size = int(0.8 * len(train_data))\nunseen_size = len(train_data) - train_size\nseen_data, unseen_data = random_split(train_data, [train_size, unseen_size])\n\ntrain_loader = DataLoader(seen_data, batch_size=64, shuffle=True)\nunseen_loader = DataLoader(unseen_data, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\ndef train_baseline_model(model, train_loader, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\nbaseline_model = CIFAR10Classifier()\nconv_output_size = get_conv_output_size(baseline_model)\nbaseline_model.fc1 = nn.Linear(conv_output_size, 64)  # Update fc1 with correct input size\nbaseline_model = train_baseline_model(baseline_model, train_loader, epochs=10)\n\ndef train_privacy_model(model, train_loader, epochs=10, lr=0.001, noise_multiplier=1.1, max_grad_norm=1.0):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Clip gradients\n            total_norm = torch.norm(torch.stack([torch.norm(p.grad) for p in model.parameters() if p.grad is not None]), 2.0)\n            clip_coef = max_grad_norm / (total_norm + 1e-6)\n            if clip_coef < 1:\n                for p in model.parameters():\n                    if p.grad is not None:\n                        p.grad.data.mul_(clip_coef)\n            \n            # Add noise to gradients\n            for p in model.parameters():\n                if p.grad is not None:\n                    noise = torch.normal(0, noise_multiplier * max_grad_norm, p.grad.shape).to(device)\n                    p.grad.add_(noise)\n            \n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\nprivacy_model = CIFAR10Classifier()\nprivacy_model.fc1 = nn.Linear(conv_output_size, 64)  # Update fc1 with correct input size\nprivacy_model = train_privacy_model(privacy_model, train_loader, epochs=10)\n\ndef generate_attack_data(model, data_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    attack_data = []\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            attack_data.append((outputs.cpu().numpy(), labels.cpu().numpy()))\n    return attack_data\n\ndef train_multiple_shadow_models(num_shadow_models, train_loader, epochs=10):\n    shadow_models = []\n    for _ in range(num_shadow_models):\n        model = CIFAR10Classifier()\n        model.fc1 = nn.Linear(conv_output_size, 64)  # Update fc1 with correct input size\n        model = train_baseline_model(model, train_loader, epochs=epochs)\n        shadow_models.append(model)\n    return shadow_models\n\ndef generate_shadow_attack_data(shadow_models, data_loader):\n    attack_data = []\n    for model in shadow_models:\n        attack_data.extend(generate_attack_data(model, data_loader))\n    return attack_data\n\nnum_shadow_models = 5  \nshadow_models_baseline = train_multiple_shadow_models(num_shadow_models, train_loader)\nshadow_models_privacy = train_multiple_shadow_models(num_shadow_models, train_loader)\n\nshadow_attack_data_baseline = generate_shadow_attack_data(shadow_models_baseline, train_loader)\nshadow_attack_data_privacy = generate_shadow_attack_data(shadow_models_privacy, train_loader)\n\ndef prepare_attack_data(seen_data, unseen_data):\n    x = []\n    y = []\n    for data in seen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(1)  \n    for data in unseen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(0)  \n    return list(zip(x, y))\n\nattack_data_baseline = prepare_attack_data(shadow_attack_data_baseline, unseen_attack_data_baseline)\nattack_data_privacy = prepare_attack_data(shadow_attack_data_privacy, unseen_attack_data_privacy)\n\ndef create_attack_loader(attack_data):\n    inputs, labels = zip(*attack_data)\n    inputs = torch.tensor(inputs, dtype=torch.float32)\n    labels = torch.tensor(labels, dtype=torch.long)\n    dataset = torch.utils.data.TensorDataset(inputs, labels)\n    return DataLoader(dataset, batch_size=64, shuffle=True)\n\nattack_loader_baseline = create_attack_loader(attack_data_baseline)\nattack_loader_privacy = create_attack_loader(attack_data_privacy)\n\nclass AttackerModel(nn.Module):\n    def __init__(self, input_dim):\n        super(AttackerModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef train_attacker_model(attack_loader, input_dim, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AttackerModel(input_dim).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(attack_loader)}\")\n\n    return model\n\ninput_dim = shadow_attack_data_baseline[0][0].shape[1]\n\nattacker_model_baseline = train_attacker_model(attack_loader_baseline, input_dim, epochs=10)\nattacker_model_privacy = train_attacker_model(attack_loader_privacy, input_dim, epochs=10)\n\ndef evaluate_attacker_model(model, attack_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\nmia_accuracy_privacy = evaluate_attacker_model(attacker_model_baseline, attack_loader_baseline)\nmia_accuracy_baseline = evaluate_attacker_model(attacker_model_privacy, attack_loader_privacy)\n\nprint(f\"MIA Accuracy for Baseline Model: {mia_accuracy_baseline}%\")\nprint(f\"MIA Accuracy for Privacy-Enhanced Model: {mia_accuracy_privacy}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-03T01:53:33.766974Z","iopub.execute_input":"2024-07-03T01:53:33.767316Z","iopub.status.idle":"2024-07-03T02:18:46.303760Z","shell.execute_reply.started":"2024-07-03T01:53:33.767284Z","shell.execute_reply":"2024-07-03T02:18:46.302755Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1, Loss: 1.766910039138794\nEpoch 2, Loss: 1.5127717491149901\nEpoch 3, Loss: 1.4160685876846313\nEpoch 4, Loss: 1.3594872522354127\nEpoch 5, Loss: 1.3121115957260132\nEpoch 6, Loss: 1.2810249942779541\nEpoch 7, Loss: 1.243545382499695\nEpoch 8, Loss: 1.219447069454193\nEpoch 9, Loss: 1.1998160769462585\nEpoch 10, Loss: 1.1696785223007202\nEpoch 1, Loss: 2.302423729324341\nEpoch 2, Loss: 2.2876622772216795\nEpoch 3, Loss: 2.2719620769500732\nEpoch 4, Loss: 2.2605060306549074\nEpoch 5, Loss: 2.256458484649658\nEpoch 6, Loss: 2.241231196594238\nEpoch 7, Loss: 2.2374693660736082\nEpoch 8, Loss: 2.233146411895752\nEpoch 9, Loss: 2.2190399843215944\nEpoch 10, Loss: 2.2270974060058593\nEpoch 1, Loss: 1.778077550315857\nEpoch 2, Loss: 1.5142680486679077\nEpoch 3, Loss: 1.412343450164795\nEpoch 4, Loss: 1.3400785613059998\nEpoch 5, Loss: 1.291143714237213\nEpoch 6, Loss: 1.2503027514457703\nEpoch 7, Loss: 1.2063611203193665\nEpoch 8, Loss: 1.1883525864601134\nEpoch 9, Loss: 1.1656913989067077\nEpoch 10, Loss: 1.1348079007148744\nEpoch 1, Loss: 1.756089108467102\nEpoch 2, Loss: 1.5053000253677369\nEpoch 3, Loss: 1.405119508934021\nEpoch 4, Loss: 1.330092747592926\nEpoch 5, Loss: 1.2719862281799317\nEpoch 6, Loss: 1.219294470500946\nEpoch 7, Loss: 1.1887391938209533\nEpoch 8, Loss: 1.1432993403434752\nEpoch 9, Loss: 1.124589327430725\nEpoch 10, Loss: 1.100491131591797\nEpoch 1, Loss: 1.8012656377792358\nEpoch 2, Loss: 1.534713385772705\nEpoch 3, Loss: 1.4230665662765503\nEpoch 4, Loss: 1.3491496284484863\nEpoch 5, Loss: 1.2973868072509767\nEpoch 6, Loss: 1.2546221053123474\nEpoch 7, Loss: 1.2163541864395142\nEpoch 8, Loss: 1.1941043553352355\nEpoch 9, Loss: 1.1629255074501037\nEpoch 10, Loss: 1.1390821646690368\nEpoch 1, Loss: 1.790466675567627\nEpoch 2, Loss: 1.5342820539474487\nEpoch 3, Loss: 1.4359235385894775\nEpoch 4, Loss: 1.3541292366027833\nEpoch 5, Loss: 1.2956765236854553\nEpoch 6, Loss: 1.2515002925872802\nEpoch 7, Loss: 1.2150699612617493\nEpoch 8, Loss: 1.1810321105003356\nEpoch 9, Loss: 1.1504899482727051\nEpoch 10, Loss: 1.1311648079872132\nEpoch 1, Loss: 1.777369218826294\nEpoch 2, Loss: 1.5283370584487914\nEpoch 3, Loss: 1.4288303075790405\nEpoch 4, Loss: 1.3566310998916626\nEpoch 5, Loss: 1.3010158992767333\nEpoch 6, Loss: 1.2549146322250366\nEpoch 7, Loss: 1.2205295941352845\nEpoch 8, Loss: 1.1866139139175416\nEpoch 9, Loss: 1.1618356278419495\nEpoch 10, Loss: 1.1332906600952148\nEpoch 1, Loss: 1.7748759899139404\nEpoch 2, Loss: 1.536062389755249\nEpoch 3, Loss: 1.4432393955230713\nEpoch 4, Loss: 1.3562687055587768\nEpoch 5, Loss: 1.3051938039779662\nEpoch 6, Loss: 1.2580083540916442\nEpoch 7, Loss: 1.2261458757400512\nEpoch 8, Loss: 1.1985792291641235\nEpoch 9, Loss: 1.1648353105545044\nEpoch 10, Loss: 1.1450799465179444\nEpoch 1, Loss: 1.750421374130249\nEpoch 2, Loss: 1.5223591524124145\nEpoch 3, Loss: 1.4071598888397217\nEpoch 4, Loss: 1.337895640182495\nEpoch 5, Loss: 1.28506416349411\nEpoch 6, Loss: 1.244841606426239\nEpoch 7, Loss: 1.2081783115386964\nEpoch 8, Loss: 1.1715792190551757\nEpoch 9, Loss: 1.1549186762809753\nEpoch 10, Loss: 1.1276474890708923\nEpoch 1, Loss: 1.8023016828536986\nEpoch 2, Loss: 1.5510497953414917\nEpoch 3, Loss: 1.4450601802825929\nEpoch 4, Loss: 1.3746212498664856\nEpoch 5, Loss: 1.31525825548172\nEpoch 6, Loss: 1.272025204372406\nEpoch 7, Loss: 1.2288392308235168\nEpoch 8, Loss: 1.2024766437530519\nEpoch 9, Loss: 1.1712104536056518\nEpoch 10, Loss: 1.1532022837638856\nEpoch 1, Loss: 1.7372293195724486\nEpoch 2, Loss: 1.4962487386703491\nEpoch 3, Loss: 1.3830066656112672\nEpoch 4, Loss: 1.3038709022521973\nEpoch 5, Loss: 1.2512724687576293\nEpoch 6, Loss: 1.2066822343826293\nEpoch 7, Loss: 1.1675089537620544\nEpoch 8, Loss: 1.134644476890564\nEpoch 9, Loss: 1.1125824892997742\nEpoch 10, Loss: 1.0840939810752868\nEpoch 1, Loss: 1.7626608989715575\nEpoch 2, Loss: 1.5085290161132812\nEpoch 3, Loss: 1.4066381607055665\nEpoch 4, Loss: 1.3245617587089538\nEpoch 5, Loss: 1.2721138783454895\nEpoch 6, Loss: 1.2269460881233216\nEpoch 7, Loss: 1.193661084651947\nEpoch 8, Loss: 1.1640321516036987\nEpoch 9, Loss: 1.1338246104240417\nEpoch 10, Loss: 1.1123829033851624\nEpoch 1, Loss: 0.1578720296397669\nEpoch 2, Loss: 0.12110481749270009\nEpoch 3, Loss: 0.10716841062519525\nEpoch 4, Loss: 0.09949399947457238\nEpoch 5, Loss: 0.09491247656323257\nEpoch 6, Loss: 0.0919129353274559\nEpoch 7, Loss: 0.08926985313588644\nEpoch 8, Loss: 0.08685491771319748\nEpoch 9, Loss: 0.08486096362743951\nEpoch 10, Loss: 0.08332413320524969\nEpoch 1, Loss: 0.004473242858293699\nEpoch 2, Loss: 0.0004922891247442703\nEpoch 3, Loss: 0.00038081221819333174\nEpoch 4, Loss: 0.0003217445504594749\nEpoch 5, Loss: 0.0003009999689604466\nEpoch 6, Loss: 0.00026563319997010594\nEpoch 7, Loss: 0.0002251039203708171\nEpoch 8, Loss: 0.00019291425944153998\nEpoch 9, Loss: 0.00017071969322425096\nEpoch 10, Loss: 0.00013134828067291082\nMIA Accuracy for Baseline Model: 96.77714285714286%\nMIA Accuracy for Privacy-Enhanced Model: 99.99809523809523%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"notice that the print of the accuracies are vice versa, but because of the limit of the time i can't run this cell one more time. the true output is :\n\nMIA Accuracy for Baseline Model: 99.99809523809523%\n\n\nMIA Accuracy for Privacy-Enhanced Model: 96.77714285714286%","metadata":{}},{"cell_type":"markdown","source":"second meethod","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\n\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        \n        # Calculate the size of the output of the conv layers\n        self._to_linear = None\n        self.convs(torch.randn(1, 3, 32, 32))\n        \n        self.fc1 = nn.Linear(self._to_linear, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def convs(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        if self._to_linear is None:\n            self._to_linear = x.view(x.size(0), -1).size(1)\n        return x\n\n    def forward(self, x):\n        x = self.convs(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\ntrain_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_size = int(0.8 * len(train_data))\nunseen_size = len(train_data) - train_size\nseen_data, unseen_data = random_split(train_data, [train_size, unseen_size])\n\ntrain_loader = DataLoader(seen_data, batch_size=64, shuffle=True)\nunseen_loader = DataLoader(unseen_data, batch_size=64, shuffle=False)\ntest_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n\ndef train_baseline_model(model, train_loader, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\nbaseline_model = CIFAR10Classifier()\nbaseline_model = train_baseline_model(baseline_model, train_loader, epochs=10)\n\ndef train_privacy_model(model, train_loader, epochs=10, lr=0.001, noise_multiplier=1.1, max_grad_norm=1.0):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            \n            # Clip gradients\n            total_norm = torch.norm(torch.stack([torch.norm(p.grad) for p in model.parameters() if p.grad is not None]), 2.0)\n            clip_coef = max_grad_norm / (total_norm + 1e-6)\n            if clip_coef < 1:\n                for p in model.parameters():\n                    if p.grad is not None:\n                        p.grad.data.mul_(clip_coef)\n            \n            # Add noise to gradients\n            for p in model.parameters():\n                if p.grad is not None:\n                    noise = torch.normal(0, noise_multiplier * max_grad_norm, p.grad.shape).to(device)\n                    p.grad.add_(noise)\n            \n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}\")\n\n    return model\n\nprivacy_model = CIFAR10Classifier()\nprivacy_model = train_privacy_model(privacy_model, train_loader, epochs=10)\n\ndef generate_attack_data(model, data_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    attack_data = []\n    with torch.no_grad():\n        for inputs, labels in data_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            attack_data.append((outputs.cpu().numpy(), labels.cpu().numpy()))\n    return attack_data\n\ndef train_multiple_shadow_models(num_shadow_models, train_loader, epochs=10):\n    shadow_models = []\n    for _ in range(num_shadow_models):\n        model = CIFAR10Classifier()\n        model = train_baseline_model(model, train_loader, epochs=epochs)\n        shadow_models.append(model)\n    return shadow_models\n\ndef generate_shadow_attack_data(shadow_models, data_loader):\n    attack_data = []\n    for model in shadow_models:\n        attack_data.extend(generate_attack_data(model, data_loader))\n    return attack_data\n\nnum_shadow_models = 5  \nshadow_models_baseline = train_multiple_shadow_models(num_shadow_models, train_loader)\nshadow_models_privacy = train_multiple_shadow_models(num_shadow_models, train_loader)\n\nshadow_attack_data_baseline = generate_shadow_attack_data(shadow_models_baseline, train_loader)\nshadow_attack_data_privacy = generate_shadow_attack_data(shadow_models_privacy, train_loader)\n\ndef prepare_attack_data(seen_data, unseen_data):\n    x = []\n    y = []\n    for data in seen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(1)  \n    for data in unseen_data:\n        for output, label in zip(*data):\n            x.append(output)\n            y.append(0)  \n    return list(zip(x, y))\n\nattack_data_baseline = prepare_attack_data(shadow_attack_data_baseline, unseen_attack_data_baseline)\nattack_data_privacy = prepare_attack_data(shadow_attack_data_privacy, unseen_attack_data_privacy)\n\ndef create_attack_loader(attack_data):\n    inputs, labels = zip(*attack_data)\n    inputs = torch.tensor(inputs, dtype=torch.float32)\n    labels = torch.tensor(labels, dtype=torch.long)\n    dataset = torch.utils.data.TensorDataset(inputs, labels)\n    return DataLoader(dataset, batch_size=64, shuffle=True)\n\nattack_loader_baseline = create_attack_loader(attack_data_baseline)\nattack_loader_privacy = create_attack_loader(attack_data_privacy)\n\n# Train attacker models\nclass AttackerModel(nn.Module):\n    def __init__(self, input_dim):\n        super(AttackerModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ndef train_attacker_model(attack_loader, input_dim, epochs=10, lr=0.001):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AttackerModel(input_dim).to(device)\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(attack_loader)}\")\n\n    return model\n\ninput_dim = shadow_attack_data_baseline[0][0].shape[1]\n\nattacker_model_baseline = train_attacker_model(attack_loader_baseline, input_dim, epochs=10)\nattacker_model_privacy = train_attacker_model(attack_loader_privacy, input_dim, epochs=10)\n\ndef evaluate_attacker_model(model, attack_loader):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\nmia_accuracy_baseline = evaluate_attacker_model(attacker_model_baseline, attack_loader_baseline)\nmia_accuracy_privacy = evaluate_attacker_model(attacker_model_privacy, attack_loader_privacy)\n\nprint(f\"Improved MIA Accuracy for Baseline Model: {mia_accuracy_baseline}%\")\nprint(f\"Improved MIA Accuracy for Privacy-Enhanced Model: {mia_accuracy_privacy}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-02T22:10:44.745097Z","iopub.execute_input":"2024-07-02T22:10:44.745470Z","iopub.status.idle":"2024-07-02T22:35:28.235839Z","shell.execute_reply.started":"2024-07-02T22:10:44.745430Z","shell.execute_reply":"2024-07-02T22:35:28.234874Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\nEpoch 1, Loss: 1.815518768310547\nEpoch 2, Loss: 1.5492673496246339\nEpoch 3, Loss: 1.4341563167572022\nEpoch 4, Loss: 1.3613821216583253\nEpoch 5, Loss: 1.3066818266868592\nEpoch 6, Loss: 1.2582828482627868\nEpoch 7, Loss: 1.2230890037536621\nEpoch 8, Loss: 1.198339196205139\nEpoch 9, Loss: 1.1705496068000794\nEpoch 10, Loss: 1.1518010720252991\nEpoch 1, Loss: 2.301245771026611\nEpoch 2, Loss: 2.3038307456970215\nEpoch 3, Loss: 2.3047791275024414\nEpoch 4, Loss: 2.301388904571533\nEpoch 5, Loss: 2.2973534507751463\nEpoch 6, Loss: 2.291410094833374\nEpoch 7, Loss: 2.2839398502349852\nEpoch 8, Loss: 2.2704434707641603\nEpoch 9, Loss: 2.2699807048797607\nEpoch 10, Loss: 2.2728657932281493\nEpoch 1, Loss: 1.7501491132736207\nEpoch 2, Loss: 1.489423397064209\nEpoch 3, Loss: 1.38676729221344\nEpoch 4, Loss: 1.3164008008003234\nEpoch 5, Loss: 1.2622181720733643\nEpoch 6, Loss: 1.217936426448822\nEpoch 7, Loss: 1.1830250049591065\nEpoch 8, Loss: 1.1441483530044556\nEpoch 9, Loss: 1.1183506200790405\nEpoch 10, Loss: 1.0938936970710755\nEpoch 1, Loss: 1.8040436841964722\nEpoch 2, Loss: 1.5588433061599731\nEpoch 3, Loss: 1.4495479148864747\nEpoch 4, Loss: 1.368153572654724\nEpoch 5, Loss: 1.3154953310012818\nEpoch 6, Loss: 1.2687283926963806\nEpoch 7, Loss: 1.2335726209640503\nEpoch 8, Loss: 1.190072696018219\nEpoch 9, Loss: 1.1617988955497742\nEpoch 10, Loss: 1.143175002002716\nEpoch 1, Loss: 1.7477175659179687\nEpoch 2, Loss: 1.4866198764801026\nEpoch 3, Loss: 1.3817293733596803\nEpoch 4, Loss: 1.3109526167869567\nEpoch 5, Loss: 1.2534448469161987\nEpoch 6, Loss: 1.2160201293945312\nEpoch 7, Loss: 1.1721967707633971\nEpoch 8, Loss: 1.1351187865257264\nEpoch 9, Loss: 1.1099758230209351\nEpoch 10, Loss: 1.0784616486549377\nEpoch 1, Loss: 1.7432659463882447\nEpoch 2, Loss: 1.5022016798019409\nEpoch 3, Loss: 1.3858149087905884\nEpoch 4, Loss: 1.3115682260513306\nEpoch 5, Loss: 1.2488931593894959\nEpoch 6, Loss: 1.2094894508361815\nEpoch 7, Loss: 1.1792841124534608\nEpoch 8, Loss: 1.1515894631385803\nEpoch 9, Loss: 1.1214301369667052\nEpoch 10, Loss: 1.0896271746635438\nEpoch 1, Loss: 1.764316473197937\nEpoch 2, Loss: 1.5342399673461915\nEpoch 3, Loss: 1.4465483865737916\nEpoch 4, Loss: 1.372967261505127\nEpoch 5, Loss: 1.3212955748558044\nEpoch 6, Loss: 1.26846244430542\nEpoch 7, Loss: 1.2238839156150818\nEpoch 8, Loss: 1.201505754852295\nEpoch 9, Loss: 1.1631098034858705\nEpoch 10, Loss: 1.1386011313438416\nEpoch 1, Loss: 1.778159567451477\nEpoch 2, Loss: 1.5216695287704467\nEpoch 3, Loss: 1.4183692310333251\nEpoch 4, Loss: 1.344056750869751\nEpoch 5, Loss: 1.2849955968856812\nEpoch 6, Loss: 1.2432509923934936\nEpoch 7, Loss: 1.2058297575950623\nEpoch 8, Loss: 1.1758516566276551\nEpoch 9, Loss: 1.1508700699806214\nEpoch 10, Loss: 1.1285491298675536\nEpoch 1, Loss: 1.7607485174179076\nEpoch 2, Loss: 1.5137670177459717\nEpoch 3, Loss: 1.4085309099197387\nEpoch 4, Loss: 1.3279213526725768\nEpoch 5, Loss: 1.275865944671631\nEpoch 6, Loss: 1.228521378517151\nEpoch 7, Loss: 1.1863255664825438\nEpoch 8, Loss: 1.1561672432899475\nEpoch 9, Loss: 1.1263214544296265\nEpoch 10, Loss: 1.1022408732414246\nEpoch 1, Loss: 1.807113508605957\nEpoch 2, Loss: 1.570840125656128\nEpoch 3, Loss: 1.4658553829193115\nEpoch 4, Loss: 1.3840047293663025\nEpoch 5, Loss: 1.33000068025589\nEpoch 6, Loss: 1.2882071645736695\nEpoch 7, Loss: 1.2571935480117797\nEpoch 8, Loss: 1.225228879737854\nEpoch 9, Loss: 1.1940258183479309\nEpoch 10, Loss: 1.1723761974334717\nEpoch 1, Loss: 1.796705776977539\nEpoch 2, Loss: 1.5400482093811034\nEpoch 3, Loss: 1.4434557182312011\nEpoch 4, Loss: 1.3817952239990234\nEpoch 5, Loss: 1.3252538299560548\nEpoch 6, Loss: 1.2930212670326233\nEpoch 7, Loss: 1.2641529441833497\nEpoch 8, Loss: 1.2380512392997742\nEpoch 9, Loss: 1.219628905391693\nEpoch 10, Loss: 1.1954704679489137\nEpoch 1, Loss: 1.8014683376312255\nEpoch 2, Loss: 1.5700172563552857\nEpoch 3, Loss: 1.4677477918624877\nEpoch 4, Loss: 1.3990159128189088\nEpoch 5, Loss: 1.346711771774292\nEpoch 6, Loss: 1.310407823562622\nEpoch 7, Loss: 1.2705373695373534\nEpoch 8, Loss: 1.2435141889572143\nEpoch 9, Loss: 1.2104126294136048\nEpoch 10, Loss: 1.189718437576294\nEpoch 1, Loss: 0.14843373416311753\nEpoch 2, Loss: 0.10915309779367556\nEpoch 3, Loss: 0.09673384409868893\nEpoch 4, Loss: 0.09065861351609839\nEpoch 5, Loss: 0.08705604842328793\nEpoch 6, Loss: 0.08424852547672221\nEpoch 7, Loss: 0.08171062442541603\nEpoch 8, Loss: 0.07979739091721165\nEpoch 9, Loss: 0.07839818155273255\nEpoch 10, Loss: 0.07674672806097477\nEpoch 1, Loss: 0.004770919706284464\nEpoch 2, Loss: 0.0005487957868816661\nEpoch 3, Loss: 0.00038442578110183\nEpoch 4, Loss: 0.000366273458270535\nEpoch 5, Loss: 0.0003037547107653371\nEpoch 6, Loss: 0.00022288673614025164\nEpoch 7, Loss: 0.00021842239262983646\nEpoch 8, Loss: 0.00021725211478763288\nEpoch 9, Loss: 0.00021124174778751085\nEpoch 10, Loss: 0.0001752368036018176\nImproved MIA Accuracy for Baseline Model: 97.2195238095238%\nImproved MIA Accuracy for Privacy-Enhanced Model: 99.99428571428571%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"notice that the print of the accuracies are vice versa, but because of the limit of the time i can't run this cell one more time. the true output is :\n\nMIA Accuracy for Baseline Model: 99.99428571428571%\n\n\nMIA Accuracy for Privacy-Enhanced Model: 97.2195238095238%","metadata":{}},{"cell_type":"markdown","source":"<h4 align=\"center\">Save the attacker models for presentation attack on dataset</h4>\n","metadata":{}},{"cell_type":"code","source":"# Saving the attacker models\ntorch.save(attacker_model_baseline.state_dict(), 'attacker_model_baseline.pth')\ntorch.save(attacker_model_privacy.state_dict(), 'attacker_model_privacy.pth')","metadata":{"execution":{"iopub.status.busy":"2024-07-02T23:12:39.036881Z","iopub.execute_input":"2024-07-02T23:12:39.037601Z","iopub.status.idle":"2024-07-02T23:12:39.047143Z","shell.execute_reply.started":"2024-07-02T23:12:39.037568Z","shell.execute_reply":"2024-07-02T23:12:39.046239Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<h4 align=\"center\">Simulation Question 8</h4>\n","metadata":{}},{"cell_type":"code","source":"class BinaryClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\ninput_dim = combined_features.size(1)\nattacker_model_baseline = BinaryClassifier(input_dim).to(device)\nattacker_model_privacy = BinaryClassifier(input_dim).to(device)\n\nattacker_model_baseline.load_state_dict(torch.load('/kaggle/working/attacker_model_baseline.pth', map_location=device))\nattacker_model_privacy.load_state_dict(torch.load('/kaggle/working/attacker_model_privacy.pth', map_location=device))\n\ndef evaluate_attacker_model(model, dataloader):\n    model.eval()\n    all_labels = []\n    all_predicted = []\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for features, labels in dataloader:\n            features, labels = features.to(device), labels.to(device).float().unsqueeze(1)\n            outputs = model(features).squeeze()\n            predicted = (outputs > 0.5).float()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_labels.extend(labels.cpu().numpy())\n            all_predicted.extend(predicted.cpu().numpy())\n\n    accuracy = correct / total\n    cm = confusion_matrix(all_labels, all_predicted)\n    precision = precision_score(all_labels, all_predicted)\n    recall = recall_score(all_labels, all_predicted)\n    f1 = f1_score(all_labels, all_predicted)\n\n    return accuracy, cm, precision, recall, f1\n\naccuracy_baseline, cm_baseline, precision_baseline, recall_baseline, f1_baseline = evaluate_attacker_model(attacker_model_baseline, new_loader)\nprint(f'Baseline Attacker Model:')\nprint(f'Training Accuracy: {accuracy_baseline:.4f}')\nprint(f'Confusion Matrix:\\n{cm_baseline}')\nprint(f'Precision: {precision_baseline:.4f}')\nprint(f'Recall: {recall_baseline:.4f}')\nprint(f'F1 Score: {f1_baseline:.4f}')\n\naccuracy_privacy, cm_privacy, precision_privacy, recall_privacy, f1_privacy = evaluate_attacker_model(attacker_model_privacy, new_loader)\nprint(f'Privacy-Enhanced Attacker Model:')\nprint(f'Training Accuracy: {accuracy_privacy:.4f}')\nprint(f'Confusion Matrix:\\n{cm_privacy}')\nprint(f'Precision: {precision_privacy:.4f}')\nprint(f'Recall: {recall_privacy:.4f}')\nprint(f'F1 Score: {f1_privacy:.4f}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, Subset, TensorDataset\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.linear_model import LogisticRegression\n\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(32 * 6 * 6, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = CIFAR10Classifier()\nstate_dict = torch.load(\"model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\nmodel.to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nDATA_ROOT = './cifar10'\nBATCH_SIZE = 64\n\nindices_file = 'list.txt'\nwith open(indices_file, 'r') as f:\n    indices = [int(line.strip()) for line in f]\n\nfull_train_dataset = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n\ntrain_indices_set = set(indices)\nall_indices = set(range(len(full_train_dataset)))\nother_indices = list(all_indices - train_indices_set)\n\ntrain_dataset = Subset(full_train_dataset, indices[:len(indices)//2])\nother_dataset = Subset(full_train_dataset, other_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\nother_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ntrain_labels = torch.ones(len(train_dataset)).to(device)\nother_labels = torch.zeros(len(other_dataset)).to(device)\ntest_labels = torch.zeros(len(test_dataset)).to(device)\n\ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, _ = data\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs)\n    return torch.cat(features).to(device)\n\ntrain_features = extract_features(model, train_loader)\nother_features = extract_features(model, other_loader)\ntest_features = extract_features(model, test_loader)\n\ncombined_features = torch.cat((train_features, other_features, test_features))\ncombined_labels = torch.cat((train_labels, other_labels, test_labels))\n\nnew_dataset = TensorDataset(combined_features, combined_labels)\nnew_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nclass BinaryClassifier(nn.Module):\n    def __init__(self, input_dim):\n        super(BinaryClassifier, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 1)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = torch.sigmoid(self.fc3(x))\n        return x\n\ninput_dim = combined_features.size(1)\nbinary_classifier = BinaryClassifier(input_dim).to(device)\n\ndef train_attacker_model(model, dataloader, epochs=10, lr=0.001):\n    criterion = nn.BCELoss()\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    for epoch in range(epochs):\n        model.train()\n        running_loss = 0.0\n        for features, labels in dataloader:\n            features, labels = features.to(device), labels.to(device).float().unsqueeze(1)\n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {running_loss / len(dataloader)}\")\n\n    return model\n\nbinary_classifier = train_attacker_model(binary_classifier, new_loader, epochs=10)\n\nbinary_classifier.eval()\nall_labels = []\nall_predicted = []\ncorrect = 0\ntotal = 0\n\nwith torch.no_grad():\n    for features, labels in new_loader:\n        features, labels = features.to(device), labels.to(device).float().unsqueeze(1)\n        outputs = binary_classifier(features).squeeze()\n        predicted = (outputs > 0.5).float()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n        all_labels.extend(labels.cpu().numpy())\n        all_predicted.extend(predicted.cpu().numpy())\n\naccuracy = correct / total\nprint(f'Training Accuracy: {accuracy:.4f}')\n\ncm = confusion_matrix(all_labels, all_predicted)\nprecision = precision_score(all_labels, all_predicted)\nrecall = recall_score(all_labels, all_predicted)\nf1 = f1_score(all_labels, all_predicted)\n\nprint(f'Confusion Matrix:\\n{cm}')\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\nprint(f\"F1 Score: {f1:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset, TensorDataset\nfrom torchvision import datasets, transforms\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n\n# Define the CIFAR10Classifier model\nclass CIFAR10Classifier(nn.Module):\n    def __init__(self):\n        super(CIFAR10Classifier, self).__init__()\n        self.conv1 = nn.Conv2d(3, 16, 3, 1)\n        self.conv2 = nn.Conv2d(16, 32, 3, 1)\n        self.dropout1 = nn.Dropout2d(0.25)\n        self.dropout2 = nn.Dropout2d(0.5)\n        self.fc1 = nn.Linear(6272, 64)\n        self.fc2 = nn.Linear(64, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\n# Load the unknown model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CIFAR10Classifier()\nstate_dict = torch.load(\"model_state_dict.pth\", map_location=device)\nnew_state_dict = {key.replace('_module.', ''): value for key, value in state_dict.items()}\nmodel.load_state_dict(new_state_dict)\nmodel.to(device)\nmodel.eval()\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\nDATA_ROOT = '../cifar10'\nBATCH_SIZE = 64\n\nindices_file = 'list.txt'\nwith open(indices_file, 'r') as f:\n    indices = [int(line.strip()) for line in f]\n\nfull_train_dataset = datasets.CIFAR10(root=DATA_ROOT, train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root=DATA_ROOT, train=False, download=True, transform=transform)\n\ntrain_indices_set = set(indices)\nall_indices = set(range(len(full_train_dataset)))\nother_indices = list(all_indices - train_indices_set)\n\ntrain_dataset = Subset(full_train_dataset, indices[:len(indices)//2])\nother_dataset = Subset(full_train_dataset, other_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\nother_loader = DataLoader(other_dataset, batch_size=BATCH_SIZE, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\ntrain_labels = torch.ones(len(train_dataset)).to(device)\nother_labels = torch.zeros(len(other_dataset)).to(device)\ntest_labels = torch.zeros(len(test_dataset)).to(device)\n\ndef extract_features(model, dataloader):\n    model.eval()\n    features = []\n    with torch.no_grad():\n        for data in dataloader:\n            inputs, _ = data\n            inputs = inputs.to(device)\n            outputs = model(inputs)\n            features.append(outputs)\n    return torch.cat(features).to(device)\n\ntrain_features = extract_features(model, train_loader)\nother_features = extract_features(model, other_loader)\ntest_features = extract_features(model, test_loader)\n\ncombined_features = torch.cat((train_features, other_features, test_features))\ncombined_labels = torch.cat((train_labels, other_labels, test_labels))\n\nnew_dataset = TensorDataset(combined_features, combined_labels)\nnew_loader = DataLoader(new_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nclass AttackerModel(nn.Module):\n    def __init__(self, input_dim):\n        super(AttackerModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, 128)\n        self.fc2 = nn.Linear(128, 64)\n        self.fc3 = nn.Linear(64, 2)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\ninput_dim = train_features.shape[1]\nattacker_model_baseline = AttackerModel(input_dim).to(device)\nattacker_model_privacy = AttackerModel(input_dim).to(device)\n\nattacker_model_baseline.load_state_dict(torch.load('attacker_model_baseline.pth'))\nattacker_model_privacy.load_state_dict(torch.load('attacker_model_privacy.pth'))\n\ndef evaluate_attacker_model(model, attack_loader):\n    model.eval()\n    correct = 0\n    total = 0\n    all_labels = []\n    all_predicted = []\n    with torch.no_grad():\n        for inputs, labels in attack_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            all_labels.extend(labels.cpu().numpy())\n            all_predicted.extend(predicted.cpu().numpy())\n    accuracy = 100 * correct / total\n    cm = confusion_matrix(all_labels, all_predicted)\n    precision = precision_score(all_labels, all_predicted)\n    recall = recall_score(all_labels, all_predicted)\n    f1 = f1_score(all_labels, all_predicted)\n    return accuracy, cm, precision, recall, f1\n\nmia_accuracy_baseline, cm_baseline, precision_baseline, recall_baseline, f1_baseline = evaluate_attacker_model(attacker_model_baseline, new_loader)\nmia_accuracy_privacy, cm_privacy, precision_privacy, recall_privacy, f1_privacy = evaluate_attacker_model(attacker_model_privacy, new_loader)\n\nprint(f\"MIA Accuracy for Baseline Attacker Model: {mia_accuracy_baseline}%\")\nprint(f\"Confusion Matrix for Baseline Attacker Model:\\n{cm_baseline}\")\nprint(f\"Precision for Baseline Attacker Model: {precision_baseline:.4f}\")\nprint(f\"Recall for Baseline Attacker Model: {recall_baseline:.4f}\")\nprint(f\"F1 Score for Baseline Attacker Model: {f1_baseline:.4f}\")\n\nprint(f\"MIA Accuracy for Privacy-Enhanced Attacker Model: {mia_accuracy_privacy}%\")\nprint(f\"Confusion Matrix for Privacy-Enhanced Attacker Model:\\n{cm_privacy}\")\nprint(f\"Precision for Privacy-Enhanced Attacker Model: {precision_privacy:.4f}\")\nprint(f\"Recall for Privacy-Enhanced Attacker Model: {recall_privacy:.4f}\")\nprint(f\"F1 Score for Privacy-Enhanced Attacker Model: {f1_privacy:.4f}\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}